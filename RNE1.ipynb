{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 13:55:22.332642: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-23 13:55:22.333975: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-23 13:55:22.337833: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-23 13:55:22.347905: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761195322.364745 3572790 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761195322.369807 3572790 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761195322.383348 3572790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761195322.383363 3572790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761195322.383364 3572790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761195322.383366 3572790 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-23 13:55:22.387821: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import signal\n",
    "from random import sample\n",
    "import tensorflow as tf\n",
    "import os, time, json\n",
    "from math import asin\n",
    "from random import choice, choices\n",
    "from shutil import copyfile\n",
    "\n",
    "def loadArrayInt(fn, usecols=None):\n",
    "    return np.array(pd.read_csv(fn, sep=' ', header=None, usecols=usecols, dtype=np.int32))\n",
    "def save_csv(fn, data):\n",
    "    pd.DataFrame(data).to_csv(fn, sep=' ', header=0, index=False)\n",
    "\n",
    "def read_node(fn):\n",
    "    with open(fn, 'r') as fp:\n",
    "        n_node, n_edge = [int(x) for x in fp.readline()[:-1].split(' ')[:2]]\n",
    "    edges = np.array(pd.read_csv(fn, sep=' ', header=None, skiprows=1, dtype=np.int32))\n",
    "    print(n_node, n_edge)\n",
    "    assert(n_edge == edges.shape[0])\n",
    "    return edges, n_node\n",
    "def partDict(parts_):\n",
    "    part_dict, cnt = {}, 0\n",
    "    for part_ in parts_:\n",
    "        if(not part_ in part_dict):\n",
    "            part_dict[part_] = cnt\n",
    "            cnt += 1\n",
    "    parts_ = np.array([part_dict[part_] for part_ in parts_])\n",
    "#     print(\"get parts\", len(part_dict), np.max(parts_))\n",
    "    return parts_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RN, F_man_dim = \"BJ\", 64\n",
    "F_n_train, F_n_val = int(1e7), int(1e6)\n",
    "F_batch_size, F_learning_rate = 4096, 4e-2\n",
    "\n",
    "SaveEmb, n_part_left = 1, 8\n",
    "dir_road, dir_data = \"./data/\" + RN + '/', \"./train/\" + RN + '/'\n",
    "f_edge, = [dir_road+x for x in [RN+'.gr']]\n",
    "f_part_full = dir_road+\"Nodes_full_4_%d.data\"%n_part_left\n",
    "F_train_dir, f_log, f_stat = dir_data+\"model/\", dir_data+\"log.out\", dir_data+\"stat\"\n",
    "f_train, f_test = [dir_data + \"%s.data\"%x for x in ['train', 'test']]\n",
    "f_emb_output = dir_road + \"emb%d_\"%(F_man_dim)\n",
    "if(not os.path.exists(F_train_dir)):\n",
    "    os.mkdir(F_train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338024 881050\n",
      "nodes: (338024, 2) edges: (881050, 3) parts: (338024, 12)\n"
     ]
    }
   ],
   "source": [
    "edges_g, n_node = read_node(f_edge)\n",
    "nodes = np.zeros((n_node, 2))\n",
    "parts = loadArrayInt(f_part_full)[:, 1::2]\n",
    "print(\"nodes:\", nodes.shape, \"edges:\", edges_g.shape, \"parts:\", parts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for data normalization\n",
    "def toStd(data):\n",
    "    return (data - mean_train) / std_train\n",
    "def fromStd(data):\n",
    "    return data * std_train + mean_train\n",
    "\n",
    "# train, valid and infer\n",
    "def run_epoch(model, sess, idxs, y, istrain=True, bs=4096): # Training Process\n",
    "    loss, loss_step = [np.array([0.0]*3) for x in range(2)]\n",
    "    st, ed, times, step = 0, bs, 0, 2000000\n",
    "    time_step, times_old = step, 0\n",
    "    while st < idxs.shape[0] and ed <= idxs.shape[0]:\n",
    "        X_batch, y_batch = idxs[st:ed], y[st:ed]\n",
    "        feed = {model.x_: X_batch, model.y_: toStd(y_batch)}\n",
    "        if(istrain):\n",
    "            loss_, loss_abs_, loss_rel_, _ = sess.run([model.loss, model.loss_abs, model.loss_rel, model.train_op], feed)\n",
    "        else:\n",
    "            loss_, loss_abs_, loss_rel_ = sess.run([model.loss_val, model.loss_abs_val, model.loss_rel_val], feed)\n",
    "        loss += np.array([loss_, loss_abs_, loss_rel_])\n",
    "        st, ed, times = ed, ed+bs, times+1\n",
    "        if(times * bs >= time_step):\n",
    "            loss_step_, loss_step[:] = loss - loss_step, loss[:]\n",
    "            mean_step = np.mean(y[times_old*bs: times*bs])\n",
    "            num_, times_old = times - times_old, times\n",
    "            loss_step_ /= num_\n",
    "            print(\"(%d): %.5f,%.5f\"%(times * bs, loss_step_[1], loss_step_[2]))\n",
    "            time_step += step\n",
    "    return loss / times\n",
    "\n",
    "def inference(model, sess, idxs): # Test Process\n",
    "    start_time = time.time()\n",
    "    pred = sess.run(model.pred_val, {model.x_: idxs})\n",
    "    pred_ = fromStd(pred).astype(np.int)\n",
    "    during_time = time.time() - start_time\n",
    "    print(\"pred(%d)data: %d(mS)\"%(pred_.shape[0], int(during_time*1000)))\n",
    "    return pred_, np.mean(pred_)\n",
    "\n",
    "# save/restore model\n",
    "def save_emb(model, sess, fid):\n",
    "    np.save(F_train_dir+\"emb%08d\"%fid, mlp_model.embedding_i.eval())\n",
    "    print(\"model saved to \" + F_train_dir+\"emb%08d\"%fid)\n",
    "def load_emb(model, sess, fid):\n",
    "    sess.run(mlp_model.emb_ass, feed_dict={mlp_model.emb_new: np.load(F_train_dir+\"emb%08d.npy\"%fid)})\n",
    "def save_emb_int(model, fid, mid=0):\n",
    "    emb_ = np.load(F_train_dir+\"emb%08d.npy\"%fid)\n",
    "#     emb_ = mlp_model.embedding_i.eval()\n",
    "    emb_ = fromStd(emb_) / F_man_dim\n",
    "    save_csv(f_emb_output+str(mid), emb_.astype(np.int))\n",
    "    print(\"save to \", f_emb_output+str(mid))\n",
    "\n",
    "    \n",
    "# map training samples into hier level\n",
    "def getIdx(idxs, Transform=False, parts_=None):\n",
    "    if(Transform): return parts_[idxs];\n",
    "    else: return idxs;\n",
    "    \n",
    "def get_train_data(num, Transform, parts_, w):\n",
    "    global F_n_train, idxs_val, y_val, idxs_train, y_train\n",
    "    idx_ = np.random.choice(idxs_train.shape[0], idxs_train.shape[0], replace=False)\n",
    "    idxs_train, y_train = idxs_train[idx_], y_train[idx_]\n",
    "    idxs_train_, idxs_val_ = getIdx(idxs_train, Transform, parts_), getIdx(idxs_val, Transform, parts_)\n",
    "    print(\"#training data: %d\"%(y_train.shape[0]))\n",
    "    return idxs_train_, y_train, idxs_val_, y_val\n",
    "\n",
    "def train_epochs(sess, epochs, lr, bs, Transform=False, parts_=None, w=None, learning_rate_decay=True, checkpoint_id=0):\n",
    "    pre_losses, best_val_loss, stay_cnt, last_loss = [1e18] * 3, 1e18, 0, 1e18\n",
    "    lr_ass = mlp_model.learning_rate.assign(lr)\n",
    "    sess.run(lr_ass)\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        idxs_train, y_train, idxs_val, y_val = get_train_data(F_n_train, Transform, parts_, w)\n",
    "        \n",
    "        print(\"Epoch(%d): lr= %f, bs= %d\"%(epoch, mlp_model.learning_rate.eval(), bs))\n",
    "        print(\"train: (#samples): mean abs err, mean abs rel err\")\n",
    "        train_loss = run_epoch(mlp_model, sess, idxs_train, y_train, istrain=True, bs=bs)\n",
    "        print(\"valid:\")\n",
    "        val_loss = run_epoch(mlp_model, sess, idxs_val, y_val, istrain=False, bs=bs)\n",
    "\n",
    "        if val_loss[0] <= best_val_loss:  # when valid_accuracy > best_valid_accuracy, save the model\n",
    "            checkpoint_id += 1\n",
    "            save_emb(mlp_model, sess, checkpoint_id)\n",
    "            best_val_loss = val_loss[0]\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(\"Epoch \" + str(epoch) + \" of \" + str(epochs) + \" took \" + str(epoch_time) + \"s\")\n",
    "        print(\"  training loss:             %.5f,%.5f\"%tuple(train_loss[-2:]))\n",
    "        print(\"  validation loss:           %.5f,%.5f\"%tuple(val_loss[-2:]))\n",
    "        print(\"\")\n",
    "\n",
    "        if(train_loss[0] >= last_loss):\n",
    "            if(learning_rate_decay): stay_cnt += 1\n",
    "            if(stay_cnt >= 3): sess.run(mlp_model.learning_rate_decay_op); stay_cnt = 0; last_loss = train_loss[0]\n",
    "        else: stay_cnt = 0; last_loss = train_loss[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9500000 1000000 0 49960.952911157896\n"
     ]
    }
   ],
   "source": [
    "data_train = loadArrayInt(f_train)\n",
    "idxs_train, y_train = data_train[:, :2], data_train[:, 2]\n",
    "mean_train, std_train = np.mean(y_train), np.std(y_train)\n",
    "mean_train, std_train = 0, mean_train\n",
    "\n",
    "data_val = loadArrayInt(f_test)\n",
    "idxs_val, y_val = data_val[:, :2], data_val[:, 2]\n",
    "print(idxs_train.shape[0], idxs_val.shape[0], mean_train, std_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/guangcan/miniconda3/envs/rne/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2, 64)\n",
      "(?,)\n",
      "WARNING:tensorflow:From /home/guangcan/miniconda3/envs/rne/lib/python3.10/site-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "model inited\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "class Model:\n",
    "    def __init__(self, learning_rate=0.04, learning_rate_decay_factor=0.8):\n",
    "        self.y_ = tf.placeholder(tf.float32, [None])\n",
    "        self.x_ = tf.placeholder(tf.int32, [None, 2])\n",
    "        self.initializer_uniform = tf.initializers.random_uniform(-3/2, 3/2)\n",
    "        \n",
    "        self.loss, self.pred, self.loss_abs, self.loss_rel = self.forward(True)\n",
    "        self.loss_val, self.pred_val, self.loss_abs_val, self.loss_rel_val = self.forward(False, reuse=True)\n",
    "\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False, dtype=tf.float32)\n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * learning_rate_decay_factor)\n",
    "        self.emb_new = tf.placeholder(tf.float32, shape=self.embedding_i.shape)\n",
    "        self.emb_ass = self.embedding_i.assign(self.emb_new)\n",
    "\n",
    "        self.params = tf.trainable_variables()\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)# update the BN params when training\n",
    "\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer = tf.train.RMSPropOptimizer(self.learning_rate, 0.9, 0.9)\n",
    "            gvs = optimizer.compute_gradients(self.loss)\n",
    "            self.train_op = optimizer.apply_gradients(gvs)\n",
    "        self.saver = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2,\n",
    "                                    max_to_keep=5, pad_step_number=True, keep_checkpoint_every_n_hours=1.0)\n",
    "        print(\"model inited\")\n",
    "                                    \n",
    "    def forward(self, is_train, reuse=None):\n",
    "        with tf.variable_scope(\"model\", reuse=reuse):\n",
    "            self.embedding_i = tf.get_variable(\"embedding_i\", [nodes.shape[0], F_man_dim], trainable=is_train, initializer=self.initializer_uniform)\n",
    "            xs_em = tf.nn.embedding_lookup(self.embedding_i, self.x_)#shape: (bs, 2, l_emb)\n",
    "            if(is_train): print(xs_em.shape)\n",
    "            dx_ = tf.abs(xs_em[:, 0, :] - xs_em[:, 1, :])# emb1 - emb2\n",
    "            pred = tf.reduce_mean(dx_, axis=1) #expect when w=1, pred and y_ are in the same level\n",
    "\n",
    "        if(is_train): print(pred.shape)\n",
    "        df_ = pred - self.y_\n",
    "        loss, loss_abs = tf.reduce_mean(tf.square(df_)), tf.reduce_mean(tf.abs(df_))\n",
    "        \n",
    "        y_, pred_ = fromStd(self.y_), fromStd(pred)\n",
    "        diff_abs_ = tf.abs(pred_ - y_)\n",
    "        diff_abs = tf.reduce_mean(diff_abs_)\n",
    "        diff_rel = tf.reduce_mean(diff_abs_ / tf.maximum(y_, 1.0))#(y_ + F_rel_fac))\n",
    "        return loss, pred, diff_abs, diff_rel\n",
    "\n",
    "mlp_model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(66601)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check #nodes in level $lev\n",
    "lev = 8\n",
    "np.max(partDict(parts[:, lev])) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch, n_hier, ptsIdxs = 10, 6, np.array([4, 5, 6, 7, 8, 11])\n",
    "epochs, bss, lrs = np.array([1, 1, 1, 1, 1, n_epoch]), np.array([F_batch_size]*n_hier), np.array([F_learning_rate]*n_hier)\n",
    "embeddings = np.zeros((nodes.shape[0], F_man_dim))\n",
    "F_train_version = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761195329.213098 3572790 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "E0000 00:00:1761195329.214923 3572790 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1761195329.222292 3572790 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1761195329.231964 3572790 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0: 256 nodes\n",
      "#training data: 9500000\n",
      "Epoch(0): lr= 0.040000, bs= 4096\n",
      "train: (#samples): mean abs err, mean abs rel err\n",
      "(2002944): 10387.41596,0.38667\n",
      "(4001792): 3811.87694,0.12098\n",
      "(6000640): 3813.27620,0.12083\n",
      "(8003584): 3811.14987,0.12058\n",
      "valid:\n",
      "model saved to ./train/BJ/model/emb00000001\n",
      "Epoch 0 of 1 took 8.459240913391113s\n",
      "  training loss:             5198.27059,0.17680\n",
      "  validation loss:           3800.50007,0.12088\n",
      "\n",
      "level 1: 1024 nodes\n",
      "#training data: 9500000\n",
      "Epoch(0): lr= 0.040000, bs= 4096\n",
      "train: (#samples): mean abs err, mean abs rel err\n",
      "(2002944): 2537.66886,0.08298\n",
      "(4001792): 2158.47081,0.07058\n",
      "(6000640): 2147.82021,0.07038\n",
      "(8003584): 2143.42515,0.06984\n",
      "valid:\n",
      "model saved to ./train/BJ/model/emb00000001\n",
      "Epoch 0 of 1 took 10.170874118804932s\n",
      "  training loss:             2230.90003,0.07292\n",
      "  validation loss:           2156.70360,0.07085\n",
      "\n",
      "level 2: 4096 nodes\n",
      "#training data: 9500000\n",
      "Epoch(0): lr= 0.040000, bs= 4096\n",
      "train: (#samples): mean abs err, mean abs rel err\n",
      "(2002944): 1527.60552,0.05164\n",
      "(4001792): 1135.75989,0.03872\n",
      "(6000640): 1128.99932,0.03833\n",
      "(8003584): 1122.86524,0.03793\n",
      "valid:\n",
      "model saved to ./train/BJ/model/emb00000001\n",
      "Epoch 0 of 1 took 13.080979824066162s\n",
      "  training loss:             1211.92922,0.04110\n",
      "  validation loss:           1119.03882,0.03786\n",
      "\n",
      "level 3: 16384 nodes\n",
      "#training data: 9500000\n",
      "Epoch(0): lr= 0.040000, bs= 4096\n",
      "train: (#samples): mean abs err, mean abs rel err\n",
      "(2002944): 971.97798,0.03367\n",
      "(4001792): 714.23069,0.02577\n",
      "(6000640): 635.76147,0.02302\n",
      "(8003584): 618.46953,0.02243\n",
      "valid:\n",
      "model saved to ./train/BJ/model/emb00000001\n",
      "Epoch 0 of 1 took 16.295243978500366s\n",
      "  training loss:             715.87508,0.02558\n",
      "  validation loss:           612.10136,0.02208\n",
      "\n",
      "level 4: 66601 nodes\n",
      "#training data: 9500000\n",
      "Epoch(0): lr= 0.040000, bs= 4096\n",
      "train: (#samples): mean abs err, mean abs rel err\n",
      "(2002944): 565.64786,0.02066\n",
      "(4001792): 541.81128,0.01987\n",
      "(6000640): 498.58202,0.01852\n",
      "(8003584): 456.04353,0.01733\n",
      "valid:\n",
      "model saved to ./train/BJ/model/emb00000001\n",
      "Epoch 0 of 1 took 19.6873459815979s\n",
      "  training loss:             501.64406,0.01865\n",
      "  validation loss:           427.25508,0.01656\n",
      "\n",
      "level 5: 338024 nodes\n",
      "#training data: 9500000\n",
      "Epoch(0): lr= 0.040000, bs= 4096\n",
      "train: (#samples): mean abs err, mean abs rel err\n",
      "(2002944): 396.94614,0.01521\n",
      "(4001792): 391.23855,0.01512\n",
      "(6000640): 389.05964,0.01508\n",
      "(8003584): 387.97379,0.01481\n",
      "valid:\n",
      "model saved to ./train/BJ/model/emb00000001\n",
      "Epoch 0 of 10 took 21.56576108932495s\n",
      "  training loss:             390.82344,0.01502\n",
      "  validation loss:           410.73022,0.01623\n",
      "\n",
      "#training data: 9500000\n",
      "Epoch(1): lr= 0.040000, bs= 4096\n",
      "train: (#samples): mean abs err, mean abs rel err\n",
      "(2002944): 378.79650,0.01451\n",
      "(4001792): 379.64792,0.01483\n",
      "(6000640): 379.81932,0.01459\n",
      "(8003584): 379.94456,0.01461\n",
      "valid:\n",
      "model saved to ./train/BJ/model/emb00000002\n",
      "Epoch 1 of 10 took 21.41547155380249s\n",
      "  training loss:             379.46702,0.01462\n",
      "  validation loss:           407.42642,0.01622\n",
      "\n",
      "#training data: 9500000\n",
      "Epoch(2): lr= 0.040000, bs= 4096\n",
      "train: (#samples): mean abs err, mean abs rel err\n",
      "(2002944): 370.09236,0.01431\n",
      "(4001792): 367.22379,0.01403\n",
      "(6000640): 363.42211,0.01437\n",
      "(8003584): 356.33304,0.01404\n",
      "valid:\n",
      "model saved to ./train/BJ/model/emb00000003\n",
      "Epoch 2 of 10 took 21.288921356201172s\n",
      "  training loss:             361.57080,0.01412\n",
      "  validation loss:           376.05352,0.01552\n",
      "\n",
      "#training data: 9500000\n",
      "Epoch(3): lr= 0.040000, bs= 4096\n",
      "train: (#samples): mean abs err, mean abs rel err\n",
      "(2002944): 321.76241,0.01288\n",
      "(4001792): 309.57691,0.01267\n",
      "(6000640): 297.25883,0.01220\n",
      "(8003584): 286.84728,0.01214\n",
      "valid:\n",
      "model saved to ./train/BJ/model/emb00000004\n",
      "Epoch 3 of 10 took 21.449835062026978s\n",
      "  training loss:             300.10489,0.01240\n",
      "  validation loss:           309.35896,0.01378\n",
      "\n",
      "#training data: 9500000\n",
      "Epoch(4): lr= 0.040000, bs= 4096\n",
      "train: (#samples): mean abs err, mean abs rel err\n",
      "(2002944): 249.33654,0.01059\n",
      "(4001792): 249.62391,0.01032\n",
      "(6000640): 249.58680,0.01047\n",
      "(8003584): 250.57806,0.01053\n",
      "valid:\n",
      "model saved to ./train/BJ/model/emb00000005\n",
      "Epoch 4 of 10 took 20.806227445602417s\n",
      "  training loss:             250.01575,0.01050\n",
      "  validation loss:           288.32628,0.01319\n",
      "\n",
      "#training data: 9500000\n",
      "Epoch(5): lr= 0.040000, bs= 4096\n",
      "train: (#samples): mean abs err, mean abs rel err\n",
      "(2002944): 217.79125,0.00883\n",
      "(4001792): 220.27175,0.00899\n",
      "(6000640): 222.33622,0.00902\n",
      "(8003584): 225.38009,0.00941\n",
      "valid:\n",
      "model saved to ./train/BJ/model/emb00000006\n",
      "Epoch 5 of 10 took 20.681648015975952s\n",
      "  training loss:             222.32184,0.00913\n",
      "  validation loss:           280.58048,0.01310\n",
      "\n",
      "#training data: 9500000\n",
      "Epoch(6): lr= 0.040000, bs= 4096\n",
      "train: (#samples): mean abs err, mean abs rel err\n",
      "(2002944): 196.75421,0.00789\n",
      "(4001792): 199.96831,0.00794\n",
      "(6000640): 202.76891,0.00829\n",
      "(8003584): 206.05888,0.00843\n",
      "valid:\n",
      "model saved to ./train/BJ/model/emb00000007\n",
      "Epoch 6 of 10 took 21.182764530181885s\n",
      "  training loss:             202.52547,0.00822\n",
      "  validation loss:           275.96221,0.01314\n",
      "\n",
      "#training data: 9500000\n",
      "Epoch(7): lr= 0.040000, bs= 4096\n",
      "train: (#samples): mean abs err, mean abs rel err\n",
      "(2002944): 181.73939,0.00711\n",
      "(4001792): 185.54362,0.00735\n",
      "(6000640): 188.66621,0.00760\n",
      "(8003584): 192.16118,0.00796\n",
      "valid:\n",
      "model saved to ./train/BJ/model/emb00000008\n",
      "Epoch 7 of 10 took 20.621772527694702s\n",
      "  training loss:             188.28920,0.00764\n",
      "  validation loss:           273.44429,0.01324\n",
      "\n",
      "#training data: 9500000\n",
      "Epoch(8): lr= 0.040000, bs= 4096\n",
      "train: (#samples): mean abs err, mean abs rel err\n",
      "(2002944): 170.95313,0.00686\n",
      "(4001792): 174.71131,0.00707\n",
      "(6000640): 178.16084,0.00722\n",
      "(8003584): 181.88280,0.00738\n",
      "valid:\n",
      "model saved to ./train/BJ/model/emb00000009\n",
      "Epoch 8 of 10 took 20.759053468704224s\n",
      "  training loss:             177.77163,0.00721\n",
      "  validation loss:           271.75886,0.01337\n",
      "\n",
      "#training data: 9500000\n",
      "Epoch(9): lr= 0.040000, bs= 4096\n",
      "train: (#samples): mean abs err, mean abs rel err\n",
      "(2002944): 162.31108,0.00634\n",
      "(4001792): 166.29480,0.00670\n",
      "(6000640): 169.78820,0.00690\n",
      "(8003584): 173.81944,0.00734\n",
      "valid:\n",
      "model saved to ./train/BJ/model/emb00000010\n",
      "Epoch 9 of 10 took 21.4316508769989s\n",
      "  training loss:             169.43629,0.00690\n",
      "  validation loss:           270.75861,0.01351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess:\n",
    "    if(F_train_version == 0):\n",
    "        tf.global_variables_initializer().run()\n",
    "    else:\n",
    "        restore_model(mlp_model, sess, F_train_version)\n",
    "    for idx in range(0, n_hier):\n",
    "        parts_ = partDict(parts[:, ptsIdxs[idx]])\n",
    "        n_p = np.max(parts_) + 1\n",
    "        print(\"level %d: %d nodes\"%(idx, n_p))\n",
    "        # input embs to model\n",
    "        if(idx != 0):#assign new embs for next training(prolongation)\n",
    "            parts__ = partDict(parts[:, ptsIdxs[idx-1]])\n",
    "            embeddings[parts_] = mlp_model.embedding_i.eval()[parts__]\n",
    "            sess.run(mlp_model.emb_ass, feed_dict={mlp_model.emb_new: embeddings})\n",
    "        #train...\n",
    "        train_epochs(sess, epochs[idx], lrs[idx], bss[idx], True, parts_, w=None, learning_rate_decay=False, checkpoint_id=F_train_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train or Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1761195610.355788 3572790 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "E0000 00:00:1761195610.357712 3572790 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1761195610.361580 3572790 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred(1000000)data: 98(mS)\n",
      "    abs: 407.388561\n",
      "    rel: 0.016208767798120263\n",
      "rel std: 0.13756290234125598\n"
     ]
    }
   ],
   "source": [
    "F_num_epochs = 100\n",
    "if not hasattr(np, \"int\"):\n",
    "    np.int = int\n",
    "if not hasattr(np, \"float\"):\n",
    "    np.float = float\n",
    "if not hasattr(np, \"bool\"):\n",
    "    np.bool = bool\n",
    "F_is_train, F_reuse_model, F_train_version, F_inference_version = 0, 0, 0, 2\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))) as sess:\n",
    "    if F_is_train:\n",
    "        if F_train_version == 0:\n",
    "            restore_model(mlp_model, sess, F_train_version)\n",
    "            lr_op = mlp_model.learning_rate.assign(F_learning_rate)\n",
    "            sess.run(lr_op)\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        train_epochs(\n",
    "            sess, F_num_epochs, F_learning_rate, F_batch_size,\n",
    "            False, checkpoint_id=F_train_version if F_reuse_model else 0\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # 推理模式\n",
    "        load_emb(mlp_model, sess, F_inference_version)\n",
    "        idxs_test, y_test = idxs_val, y_val\n",
    "\n",
    "        pred, avr = inference(mlp_model, sess, idxs_test)\n",
    "        diff_ = np.abs(pred - y_test)\n",
    "        diff_rel = diff_ / np.maximum(y_test, 1)\n",
    "\n",
    "        print(\"    abs:\", np.mean(diff_))\n",
    "        print(\"    rel:\", np.mean(diff_rel))\n",
    "        print(\"rel std:\", np.std(diff_rel))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
